{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDM_Final_Challenge_Atkins",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Our next objective is to determine the distance people traveled to grocery stores by census block group (CBG) using Safegraph data. In particular, we would like to know for each CBG, the average distance they traveled to the listed grocery stores in March 2019, October 2019, March 2020, and October 2020. (We select March and October to avoid summer and holidays with more noise from tourists and festivity shopping). To be consistent with Safegraph, we will use haversine distance for this project."
      ],
      "metadata": {
        "id": "cFWD74P1-RjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1h9e1lLMVhQp1dguiuyhOMYa_zRS6dKIn -O weekly-patterns-nyc-2019-2020-sample.csv \n",
        "!gdown --id 1w9-wx4qjwdxxVVomVYbvVTfZUATt9kmS -O nyc_supermarkets.csv\n",
        "!gdown --id 1_70Rh-j_ttFlfQ_VH3vbjnRrvcRxQdB7 -O nyc_cbg_centroids.csv\n",
        "!gdown --id 1jkhqKXzSczLyBe-n-prdwwjZypEMYcbQ -O core-places-nyc.csv\n",
        "\n",
        "!pip install pyspark\n",
        "!pip install pyproj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aYCt3lvOK6O",
        "outputId": "731d7c79-41e7-43f2-db05-b9744ff039e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h9e1lLMVhQp1dguiuyhOMYa_zRS6dKIn\n",
            "To: /content/weekly-patterns-nyc-2019-2020-sample.csv\n",
            "100% 114M/114M [00:00<00:00, 203MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1w9-wx4qjwdxxVVomVYbvVTfZUATt9kmS\n",
            "To: /content/nyc_supermarkets.csv\n",
            "100% 170k/170k [00:00<00:00, 20.5MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_70Rh-j_ttFlfQ_VH3vbjnRrvcRxQdB7\n",
            "To: /content/nyc_cbg_centroids.csv\n",
            "100% 326k/326k [00:00<00:00, 49.1MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jkhqKXzSczLyBe-n-prdwwjZypEMYcbQ\n",
            "To: /content/core-places-nyc.csv\n",
            "100% 46.3M/46.3M [00:00<00:00, 124MB/s]\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 31 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 44.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=2c0e0125f11b38bf6ab49ee139f8bed9b30fca19ec0a58bfcbed9580984b7f0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyproj\n",
            "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from pyproj) (2022.5.18.1)\n",
            "Installing collected packages: pyproj\n",
            "Successfully installed pyproj-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyproj import Proj, transform\n",
        "from math import radians, cos, sin, asin, sqrt, acos\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql import types as T\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "spark = SparkSession(sc)\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "fMINh23oS6nW",
        "outputId": "127b4d09-3257-42d9-9e30-66b6a99f17ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd1477f2090>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://de33e268452f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get a list of all the supermarkets to include\n",
        "nyc_supermarkets = spark.read.load('nyc_supermarkets.csv', format = 'csv', header = True, inferSchema = True) \\\n",
        "                  .select('safegraph_placekey').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "#list of all the dates to include, Only visit patterns where date_range_start or date_range_end overlaps with Mar 2019, Oct 2019, Mar 2020, Oct 2020\n",
        "date_list = ['2019-03', '2019-10', '2020-03', '2020-10']\n",
        "\n",
        "#filter the visits in weeklyPatterns by nyc_supermarkets and date_list\n",
        "weeklyPatterns = spark.read.option(\"header\", True).option(\"escape\", \"\\\"\").csv('weekly-patterns-nyc-2019-2020-sample.csv') \\\n",
        "          .select('placekey', \n",
        "                  'poi_cbg', \n",
        "                  'visitor_home_cbgs',\n",
        "                  F.substring('date_range_start',0,7).alias('date_range_start'),\n",
        "                  F.substring('date_range_end',0,7).alias('date_range_end')) \\\n",
        "          .filter(F.col('placekey').isin(nyc_supermarkets)) \\\n",
        "          .filter((F.col('date_range_start').isin(date_list)) |\n",
        "                  (F.col('date_range_end').isin(date_list)))\n",
        "          \n",
        "#make one date column\n",
        "weeklyPatterns = weeklyPatterns.withColumn('year_month', F.when(F.col('date_range_start').isin(date_list), F.col('date_range_start')).otherwise(F.col('date_range_end'))) \\\n",
        "                        .drop('date_range_start', 'date_range_end')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#User defined function to load json column\n",
        "def loadJson(string):\n",
        "  return json.loads(string)\n",
        "\n",
        "jsonUDF = F.udf(loadJson, T.MapType(T.StringType(), T.IntegerType()))\n",
        "\n",
        "#use the udf to convert visitor_home_cbgs to json and explode it\n",
        "weeklyPatterns = weeklyPatterns.select('placekey', \n",
        "                                      'poi_cbg', \n",
        "                                      F.explode(jsonUDF('visitor_home_cbgs')).alias('visitor_home_cbgs', 'num_visitors'),\n",
        "                                      'year_month')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#project longitutde and latidute for all cbgs to espg 2263\n",
        "cbgs_pandas = pd.read_csv('nyc_cbg_centroids.csv')\n",
        "inProj, outProj = Proj(init='epsg:4326'), Proj(init='epsg:2263')\n",
        "cbgs_pandas['newLon'], cbgs_pandas['newLat'] = transform(inProj, outProj, cbgs_pandas['longitude'].tolist(), cbgs_pandas['latitude'].tolist())\n",
        "\n",
        "nyc_cbg_centroids = spark.createDataFrame(cbgs_pandas)\n",
        "\n",
        "#inner join with nyc_cbg_centroids so that we have the projected longitude and latidude for all store cbgs\n",
        "weeklyPatterns = weeklyPatterns.join(nyc_cbg_centroids, weeklyPatterns['poi_cbg'] == nyc_cbg_centroids['cbg_fips'], 'inner') \\\n",
        "                          .withColumnRenamed('newLon', 'store_lon') \\\n",
        "                          .withColumnRenamed('newLat', 'store_lat') \\\n",
        "                          .drop('cbg_fips', 'latitude', 'longitude')\n",
        "\n",
        "#inner join with nyc_cbg_centroids so that we have the projected longitude and latidude for all visitor cbgs in weekly pattern that exist in nyc_cbg_centroids\n",
        "weeklyPatterns = weeklyPatterns.join(nyc_cbg_centroids, weeklyPatterns['visitor_home_cbgs'] == nyc_cbg_centroids['cbg_fips'], 'inner') \\\n",
        "                          .withColumnRenamed('newLon', 'visitor_lon') \\\n",
        "                          .withColumnRenamed('newLat', 'visitor_lat') \\\n",
        "                          .drop('poi_cbg', 'latitude', 'longitude')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Compute Haversince distance between visitor and store cbgs\n",
        "# def haversine_distance(start_lng, start_lat, end_lng, end_lat):\n",
        "#   start_lng, start_lat, end_lng, end_lat = map(radians, [start_lng, start_lat, end_lng, end_lat])\n",
        "#   radius = 3961 #in miles\n",
        "#   distance = (radius * acos((cos(start_lat)*cos(end_lat)*cos(end_lng-start_lng))+sin((start_lat)*sin(end_lat))))\n",
        "#   distance = distance/5280\n",
        "#   return(abs(round(distance, 2)))\n",
        "\n",
        "# udf_haversine_distance = F.udf(haversine_distance)\n",
        "\n",
        "#Compute Euclidean distances distance between visitor and store cbgs\n",
        "def euclidean_distance(start_lng, start_lat, end_lng, end_lat):\n",
        "  distance = sqrt((end_lng-start_lng)**2+(end_lat-start_lat)**2)\n",
        "  #convert from feet to miles\n",
        "  distance = distance/5280\n",
        "  return(abs(round(distance, 2)))\n",
        "\n",
        "udf_euclidean_distance = F.udf(euclidean_distance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Add column computing distance between start and end\n",
        "weeklyPatterns = weeklyPatterns.withColumn('distance', udf_euclidean_distance('visitor_lon', 'visitor_lat', 'store_lon', 'store_lat').cast('double')) \\\n",
        "                            .drop('placekey', 'visitor_home_cbgs', 'store_lon', 'store_lat', 'visitor_lon', 'visitor_lat')\n",
        "\n",
        "#grouped by cbg, pivot to create a column for each month_year with avg distance as the value\n",
        "weeklyPatterns_meanAvg = weeklyPatterns.groupBy('cbg_fips') \\\n",
        "              .pivot('year_month') \\\n",
        "              .agg(((F.sum('distance') * F.sum('num_visitors'))/F.sum('num_visitors')).alias('avg_distance')) \\\n",
        "              .drop('year_month') \\\n",
        "              .sort(F.col('cbg_fips').asc())\n",
        "\n",
        "weeklyPatterns_meanAvg.write.csv('final_challenge/final_challenge_atkins.csv', header=True)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjakE07Tckyf",
        "outputId": "5a9c5651-6989-410f-d78f-5dc334e374d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:131: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
            "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
            "/usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:131: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
            "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extra Credit 1: Compute the median traveled distances instead of the mean. \n",
        "\n",
        "weeklyPatterns_medianAvg = weeklyPatterns.withColumn('distance', F.expr('explode(array_repeat(distance,int(num_visitors)))')) \\\n",
        "              .groupBy('cbg_fips') \\\n",
        "              .pivot('year_month') \\\n",
        "              .agg((F.expr(\"percentile(distance, 0.5)\")).alias('median_distance')) \\\n",
        "              .drop('year_month') \\\n",
        "              .sort(F.col('cbg_fips').asc())\n",
        "\n",
        "weeklyPatterns_meanAvg.write.csv('final_challenge/extraCredit1_atkins.csv', header=True)"
      ],
      "metadata": {
        "id": "vOcsnsLyIqdv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extra Credit 2: Compute averge distance traveled for all grocery stores in the Safegraph data, match all placekeys with naics_code starts with 4451 in the core-places-nyc.\n",
        "#filter to only stores in nyc 5 boros\n",
        "\n",
        "core_places_nyc = spark.read.load('core-places-nyc.csv', format = 'csv', header = True, inferSchema = True) \\\n",
        "                  .filter(F.substring('naics_code',0,4) == 4451) \\\n",
        "                  .select('placekey').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "date_list = ['2019-03', '2019-10', '2020-03', '2020-10']\n",
        "nyc_cbg_list = ['36061', '36005', '36047', '36081', '36085']\n",
        "\n",
        "weeklyPatterns = spark.read.option(\"header\", True).option(\"escape\", \"\\\"\").csv('weekly-patterns-nyc-2019-2020-sample.csv') \\\n",
        "          .select('placekey', \n",
        "                  'poi_cbg', \n",
        "                  'visitor_home_cbgs',\n",
        "                  F.substring('date_range_start',0,7).alias('date_range_start'),\n",
        "                  F.substring('date_range_end',0,7).alias('date_range_end')) \\\n",
        "          .filter(F.col('placekey').isin(core_places_nyc)) \\\n",
        "          .filter(F.substring('poi_cbg',0,5).isin(nyc_cbg_list)) \\\n",
        "          .filter((F.col('date_range_start').isin(date_list)) |\n",
        "                  (F.col('date_range_end').isin(date_list)))\n",
        "          \n",
        "weeklyPatterns = weeklyPatterns.withColumn('year_month', F.when(F.col('date_range_start').isin(date_list), F.col('date_range_start')).otherwise(F.col('date_range_end'))) \\\n",
        "                        .drop('date_range_start', 'date_range_end')\n",
        "\n",
        "def loadJson(string):\n",
        "  return json.loads(string)\n",
        "\n",
        "jsonUDF = F.udf(loadJson, T.MapType(T.StringType(), T.IntegerType()))\n",
        "\n",
        "weeklyPatterns = weeklyPatterns.select('placekey', \n",
        "                                      'poi_cbg', \n",
        "                                      F.explode(jsonUDF('visitor_home_cbgs')).alias('visitor_home_cbgs', 'num_visitors'),\n",
        "                                      'year_month')\n",
        "\n",
        "cbgs_pandas = pd.read_csv('nyc_cbg_centroids.csv')\n",
        "inProj, outProj = Proj(init='epsg:4326'), Proj(init='epsg:2263')\n",
        "cbgs_pandas['newLon'], cbgs_pandas['newLat'] = transform(inProj, outProj, cbgs_pandas['longitude'].tolist(), cbgs_pandas['latitude'].tolist())\n",
        "\n",
        "nyc_cbg_centroids = spark.createDataFrame(cbgs_pandas)\n",
        "\n",
        "weeklyPatterns = weeklyPatterns.join(nyc_cbg_centroids, weeklyPatterns['poi_cbg'] == nyc_cbg_centroids['cbg_fips'], 'inner') \\\n",
        "                          .withColumnRenamed('newLon', 'store_lon') \\\n",
        "                          .withColumnRenamed('newLat', 'store_lat') \\\n",
        "                          .drop('cbg_fips', 'latitude', 'longitude')\n",
        "\n",
        "weeklyPatterns = weeklyPatterns.join(nyc_cbg_centroids, weeklyPatterns['visitor_home_cbgs'] == nyc_cbg_centroids['cbg_fips'], 'inner') \\\n",
        "                          .withColumnRenamed('newLon', 'visitor_lon') \\\n",
        "                          .withColumnRenamed('newLat', 'visitor_lat') \\\n",
        "                          .drop('poi_cbg', 'latitude', 'longitude')\n",
        "\n",
        "def euclidean_distance(start_lng, start_lat, end_lng, end_lat):\n",
        "  distance = sqrt((end_lng-start_lng)**2+(end_lat-start_lat)**2)\n",
        "  distance = distance/5280\n",
        "  return(abs(round(distance, 2)))\n",
        "\n",
        "udf_euclidean_distance = F.udf(euclidean_distance)\n",
        "\n",
        "weeklyPatterns = weeklyPatterns.withColumn('distance', udf_euclidean_distance('visitor_lon', 'visitor_lat', 'store_lon', 'store_lat').cast('double')) \\\n",
        "                            .drop('placekey', 'visitor_home_cbgs', 'store_lon', 'store_lat', 'visitor_lon', 'visitor_lat')\n",
        "\n",
        "weeklyPatterns_meanAvg = weeklyPatterns.groupBy('cbg_fips') \\\n",
        "              .pivot('year_month') \\\n",
        "              .agg(((F.sum('distance') * F.sum('num_visitors'))/F.sum('num_visitors')).alias('avg_distance')) \\\n",
        "              .drop('year_month') \\\n",
        "              .sort(F.col('cbg_fips').asc())\n",
        "\n",
        "weeklyPatterns_meanAvg.write.csv('final_challenge/extraCredit2_atkins.csv', header=True)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ilV7cFGQ0AJ",
        "outputId": "0484c3be-a08a-422f-dca6-80418fbbb92e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:131: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
            "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
            "/usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:131: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
            "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n"
          ]
        }
      ]
    }
  ]
}